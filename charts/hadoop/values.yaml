# The base hadoop image to use for all components.
# See this repo for image build details: https://github.com/Comcast/kube-yarn/tree/master/image
image:
  repository: 5200710/hadoop
  tag: 3.2.3
  pullPolicy: IfNotPresent

# Select antiAffinity as either hard or soft, default is 'soft'
# 'hard' is sugested for production setup
antiAffinity: "soft"
logLevel: INFO

conf:
  coreSite:
  # fs.trash.interval: "10080"  # trash auto purge in minutes
  hdfsSite:
    dfs.replication: 3  # when changing this value ensure that dataNode.replicas is equal or higher than this value
    # dfs.datanode.du.reserved: "4294967296"  # number of bytes to reserve on disk to block hitting disk full, must be quoted for large numbers, because of gotemplate converting large numbers to float with scientific notation
  httpfsSite: { }
  yarnSite: { }
  mapredSite: { }

# httpsfs service
httpfs:
  port: 14000
  adminPort: 14001

nameNode:
  pdbMinAvailable: 1
  port: 9820
  httpPort: 9870
  httpsPort: 9871
  service:
    type: ClusterIP
    port: 80
    annotations: { }
    # cloud.google.com/load-balancer-type: "Internal"
    loadBalancerIP: null

  resources:
    requests:
      memory: "256Mi"
      cpu: "10m"
    limits:
      memory: "2048Mi"
      cpu: "1000m"

dataNode:
  replicas: 3  # ensure this value is higher or equal to 'conf.hdfsSite.dfs.replication'
  pdbMinAvailable: 3  # ensure to set this value before deploying
  port: 9867
  httpPort: 9864
  httpsPort: 9865
  resources:
    requests:
      memory: "256Mi"
      cpu: "10m"
    limits:
      memory: "2048Mi"
      cpu: "1000m"

resourcemanager:
  replicas: 1
  port: 8088
  httpPort: 80
  httpsPort: 443
  pdbMinAvailable: 1
  service:
    type: ClusterIP
    port: 80
    annotations: { }
    # cloud.google.com/load-balancer-type: "Internal"
    loadBalancerIP: null
  resources:
    requests:
      memory: "1024Mi"
      cpu: "500m"
    limits:
      memory: "4096Mi"
      cpu: "2000m"

nodemanager:
  replicas: 3
  port: 8088
  httpPort: 8042
  httpsPort: 443
  pdbMinAvailable: 3
  resources:
    requests:
      memory: "8192Mi"
      cpu: "4000m"
    limits:
      memory: "8192Mi"
      cpu: "4000m"

historyserver:
  enabled: true
  replicas: 1
  # historyserver container port. Default HTTP port is 19888.
  port: 19888
  pdbMinAvailable: 1
  service:
    type: ClusterIP
    port: 80
    annotations: { }
    # cloud.google.com/load-balancer-type: "Internal"
    loadBalancerIP: null

ingress:
  nameNode:
    enabled: false
    annotations: {
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    }
    labels: { }
    path: /
    pathType: ImplementationSpecific
    hosts:
      - "hdfs-namenode.local"
    tls: [ ]
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
  dataNode:
    enabled: false
    annotations: {
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    }
    labels: {}
    path: /
    hosts:
    - "hdfs-datanode.local"
  httpfs:
    enabled: false
    annotations: {
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    }
    labels: { }
    path: /
    hosts:
      - "httpfs.local"
  resourcemanager:
    enabled: false
    # ingressClassName: nginx
    annotations: {
      # kubernetes.io/tls-acme: "true"
      ## Extend timeout to allow long running queries.
      # nginx.ingress.kubernetes.io/proxy-connect-timeout: "300"
      # nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
      # nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    }
    path: /
    pathType: ImplementationSpecific
    hosts:
      - "yarn.local"
    tls: [ ]
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
  historyserver:
    enabled: false
    # ingressClassName: nginx
    annotations: {
      # kubernetes.io/tls-acme: "true"
      ## Extend timeout to allow long running queries.
      # nginx.ingress.kubernetes.io/proxy-connect-timeout: "300"
      # nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
      # nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    }
    path: /
    pathType: ImplementationSpecific
    hosts:
      - "historyserver.local"
    tls: [ ]
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

persistence:
  nameNode:
    enabled: false
    storageClass:
    accessMode: ReadWriteOnce
    size: 50Gi
  dataNode:
    enabled: false
    storageClass:
    accessMode: ReadWriteOnce
    size: 200Gi


## ------------------------------------------------------
## Monitoring HDFS-NameNode
## ------------------------------------------------------

## Prometheus Exporter Configuration
## ref: https://prometheus.io/docs/instrumenting/exporters/
prometheus:
  ## Exporter Configuration
  ## ref: https://github.com/marcelmay/hadoop-hdfs-fsimage-exporter
  exporter:
    image:
      repository: marcelmay/hadoop-hdfs-fsimage-exporter
      tag: 1.3
      pullPolicy: IfNotPresent
    enabled: true
    port: 5556
